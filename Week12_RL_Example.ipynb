{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Example\n",
    "\n",
    "In this notebook, we implement a very simple model based on the Q-learning algorithm. This notebook is intend to show a basic form of RL that doesn't require a deep understanding of neural networks or advanced mathematics and how one might deploy such a model.\n",
    "\n",
    "This example shows the Grid World problem, where an agent learns to navigate a grid to reach a goal.\n",
    "\n",
    "The notebook will go through the following steps:\n",
    "\n",
    "1. Define State and Action Space\n",
    "2. Create a Q-table to store expected rewards for each state/action combination\n",
    "3. Implement learning algorithm and train model\n",
    "4. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define State and Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid settings\n",
    "grid_size = 4\n",
    "\n",
    "#funtion to build list of all state tuples\n",
    "def build_state_list(grid_size):\n",
    "    state_list = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            state_list.append((i, j))\n",
    "    return state_list\n",
    "all_states = build_state_list(grid_size)\n",
    "\n",
    "# Here we just try to reach the top right corner (could be center or any other state)\n",
    "goal_state = (3, 3)\n",
    "n_states = grid_size * grid_size\n",
    "n_actions = 4  # Up, Down, Left, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Q-table to store expected rewards for each state/action combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Helper functions\n",
    "def state_to_index(state):\n",
    "    return state[0] * grid_size + state[1]\n",
    "\n",
    "def index_to_state(index):\n",
    "    return (index // grid_size, index % grid_size)\n",
    "\n",
    "def get_possible_actions(state):\n",
    "    actions = []\n",
    "    if state[0] > 0: actions.append(0)  # Up\n",
    "    if state[0] < grid_size - 1: actions.append(1)  # Down\n",
    "    if state[1] > 0: actions.append(2)  # Left\n",
    "    if state[1] < grid_size - 1: actions.append(3)  # Right\n",
    "    return actions\n",
    "\n",
    "# Correct the state transition function to prevent invalid states\n",
    "def take_action(state, action):\n",
    "    new_state = list(state)\n",
    "    if action == 0 and state[0] > 0: new_state[0] -= 1  # Up\n",
    "    if action == 1 and state[0] < grid_size - 1: new_state[0] += 1  # Down\n",
    "    if action == 2 and state[1] > 0: new_state[1] -= 1  # Left\n",
    "    if action == 3 and state[1] < grid_size - 1: new_state[1] += 1  # Right\n",
    "    return tuple(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement learning algorithm and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1  # Exploration rate\n",
    "n_episodes = 100000\n",
    "\n",
    "# Training the model with corrected state transitions\n",
    "for episode in range(n_episodes):\n",
    "    #start at a random state\n",
    "    state = random.choice(all_states)\n",
    "    done = state == goal_state\n",
    "\n",
    "    while not done:\n",
    "        state_index = state_to_index(state)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = random.choice(get_possible_actions(state))\n",
    "        else:\n",
    "            # Exploit: choose the best action from Q-table\n",
    "            action = np.argmax(Q[state_index])\n",
    "\n",
    "        # Take action and observe reward\n",
    "        next_state = take_action(state, action)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "        next_state_index = state_to_index(next_state)\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[state_index, action] = (Q[state_index, action] + \n",
    "                                  learning_rate * (reward + \n",
    "                                  discount_factor * np.max(Q[next_state_index]) - \n",
    "                                  Q[state_index, action]))\n",
    "\n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "        done = state == goal_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will just show one path then see on average how many actions it takes to get to the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (2, 0)\n",
      "[(2, 0), (2, 1), (3, 1), (3, 2), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "state = random.choice(all_states)\n",
    "print(\"Initial state:\", state)\n",
    "trajectory = [state]\n",
    "done = state == goal_state\n",
    "while not done:\n",
    "    state_index = state_to_index(state)\n",
    "    action = np.argmax(Q[state_index])  # Choose the best action\n",
    "    state = take_action(state, action)\n",
    "    trajectory.append(state)\n",
    "    done = state == goal_state\n",
    "\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of actions taken to reach the goal: 3.0\n"
     ]
    }
   ],
   "source": [
    "total_actions = 0 # Total number of actions taken to reach the goal\n",
    "for state in all_states:\n",
    "    # Evaluating the model\n",
    "    trajectory = [state]\n",
    "    done = state == goal_state\n",
    "    while not done:\n",
    "        state_index = state_to_index(state)\n",
    "        action = np.argmax(Q[state_index])  # Choose the best action\n",
    "        state = take_action(state, action)\n",
    "        trajectory.append(state)\n",
    "        done = state == goal_state\n",
    "        total_actions += 1\n",
    "print(\"Average number of actions taken to reach the goal:\", total_actions / len(all_states))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this optimal? We know the optimal policy is to move up or to the right until we reach the goal. From the bottom left, this is 6 actions, for the next 2 states it is 5 actions, for the next 3 it is 4 actions, then 4->3, 3->2, 2->1, 1->0 as we already start at the goal state. By simple arithmetic we have\n",
    "\n",
    "6+2\\*5+3\\*4+4\\*3+3\\*2+2\\*1 = 48\n",
    "\n",
    "Total state = 16\n",
    "\n",
    "Therefore, the optimal is 48/16 = 3 which is exactly our average number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datarobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
