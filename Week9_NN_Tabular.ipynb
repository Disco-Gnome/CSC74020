{"cells":[{"cell_type":"markdown","source":["# CSC 74020 Machine Learning\n","# Week 9: Neural Networks on Tabular Data\n","\n","#### We build a few Neural Networks on tabular data and show how to use the keras model class and layer classes for building Neural Networks (including non-sequential networks)"],"metadata":{"id":"8rD50bJhCqhG"},"id":"8rD50bJhCqhG"},{"cell_type":"code","source":["!pip install tensorflow_addons"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDPZoT72zbjY","executionInfo":{"status":"ok","timestamp":1699492627362,"user_tz":300,"elapsed":16504,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"b5bb08ea-c8f1-40d3-ac0b-c2f0a8512317"},"id":"QDPZoT72zbjY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow_addons\n","Successfully installed tensorflow_addons-0.22.0 typeguard-2.13.3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5G0FonjTGJHv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699492645483,"user_tz":300,"elapsed":18125,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"eb2ca369-13f5-432a-9209-c18d14f845d1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["from typing import Any, Dict\n","\n","import numpy as np\n","import pandas as pd\n","import math\n","\n","import scipy.special\n","import sklearn.datasets\n","import sklearn.metrics\n","import sklearn.model_selection\n","import sklearn.preprocessing\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.optimizers import Adam\n","# from official.nlp import optimization\n","\n","import tensorflow_addons as tfa\n","\n","from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout, ReLU, Add, PReLU"],"id":"5G0FonjTGJHv"},{"cell_type":"markdown","metadata":{"id":"_3FQaH0zGJH2"},"source":["## Build out MLP (standard FF NN) and ResNet Block"],"id":"_3FQaH0zGJH2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysG86H8rGJH7"},"outputs":[],"source":["class MLP(tf.keras.Model):\n","    def __init__(self,  d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(MLP, self).__init__(**kwargs )\n","\n","        self.dense1 = Dense(d_main)#d_main, d_hidden, bias_first)\n","        self.dense2 = Dense(d_hidden)\n","\n","        # self.normalization = BatchNormalization()\n","        self.activation = PReLU()\n","        self.output_layer = Dense(1)\n","\n","    def call(self, inputs):\n","        x = self.dense1(inputs)\n","        x = self.activation(x)\n","        x = self.dense2(inputs)\n","        x = self.activation(x)\n","        x = self.output_layer(x)\n","        return x"],"id":"ysG86H8rGJH7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaeGX7zaGJIE"},"outputs":[],"source":["class ResNetBlock(tf.keras.layers.Layer):\n","    \"\"\"The main building block of `ResNet`.\"\"\"\n","\n","    def __init__( self, d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(ResNetBlock, self).__init__( **kwargs)\n","\n","        self.normalization = BatchNormalization()\n","        self.linear_first = Dense(d_hidden)#d_main, d_hidden, bias_first)\n","        self.activation = ReLU()\n","        self.dropout_first = Dropout(.2)\n","        self.linear_second = Dense(d_main)\n","        self.dropout_second = Dropout(0)\n","        self.skip_connection = True\n","\n","    def call(self, x):\n","        x_input = x\n","        x = self.normalization(x)\n","        x = self.linear_first(x)\n","        x = self.activation(x)\n","        x = self.dropout_first(x)\n","        x = self.linear_second(x)\n","        x = self.dropout_second(x)\n","        if self.skip_connection:\n","            x = x_input + x\n","        return x"],"id":"zaeGX7zaGJIE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpBcRYMMGJIG"},"outputs":[],"source":["class ResNet(tf.keras.Model):\n","    def __init__(self,  d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(ResNet, self).__init__(**kwargs )\n","\n","        self.linear_first = Dense(d_main)#d_main, d_hidden, bias_first)\n","        self.resnetblock1 = ResNetBlock(d_main,d_hidden)\n","        self.resnetblock2 = ResNetBlock(d_main,d_hidden)\n","        self.normalization = BatchNormalization()\n","        self.activation = PReLU()\n","        self.output_layer = Dense(1)\n","        # self.output_skip = Dense(1)\n","        # self.add_layer = Add()\n","\n","    def call(self, inputs):\n","        x = self.linear_first(inputs)\n","        #x1 = self.output_skip(inputs)\n","        x = self.resnetblock1(x)\n","        x = self.resnetblock2(x)\n","        x = self.normalization(x)\n","        x = self.activation(x)\n","        x = self.output_layer(x)\n","        #x = self.add_layer([x,x1])\n","        return x"],"id":"JpBcRYMMGJIG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_bGPtdhGJIK"},"outputs":[],"source":["class ResNetDR(tf.keras.Model):\n","    def __init__(self,  d_main: int, d_hidden: int, **kwargs ) -> None:\n","        super(ResNetDR, self).__init__(**kwargs )\n","\n","        self.dense1 = Dense(d_main)#d_main, d_hidden, bias_first)\n","        self.dense2 = Dense(d_hidden)\n","\n","        # self.normalization = BatchNormalization()\n","        self.activation = PReLU()\n","        self.output_layer = Dense(1)\n","        self.output_skip = Dense(1)\n","        self.add_layer = Add()\n","\n","    def call(self, inputs):\n","        x = self.dense1(inputs)\n","        x1 = self.output_skip(inputs)\n","        x = self.activation(x)\n","        x = self.dense2(inputs)\n","        x = self.activation(x)\n","        x = self.output_layer(x)\n","        x = self.add_layer([x,x1])\n","        return x"],"id":"1_bGPtdhGJIK"},{"cell_type":"markdown","metadata":{"id":"MxFC2QTPGJIM"},"source":["### Data"],"id":"MxFC2QTPGJIM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqdtcC36GJIN"},"outputs":[],"source":["# !!! NOTE !!! The dataset splits, preprocessing and other details are\n","# significantly different from those used in the\n","# paper \"Revisiting Deep Learning Models for Tabular Data\",\n","# so the results will be different from the reported in the paper.\n","\n","dataset = sklearn.datasets.fetch_california_housing()\n","task_type = 'regression'\n","\n","# dataset = sklearn.datasets.fetch_covtype()\n","# task_type = 'multiclass'\n","\n","assert task_type in ['binclass', 'multiclass', 'regression']\n","\n","X_all = dataset['data'].astype('float32')\n","y_all = dataset['target'].astype('float32' if task_type == 'regression' else 'int64')\n","if task_type != 'regression':\n","    y_all = sklearn.preprocessing.LabelEncoder().fit_transform(y_all).astype('int64')\n","n_classes = int(max(y_all)) + 1 if task_type == 'multiclass' else None\n","\n","X = {}\n","y = {}\n","X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n","    X_all, y_all, train_size=0.8\n",")\n","X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n","    X['train'], y['train'], train_size=0.8\n",")\n","\n","X_orig=X.copy()"],"id":"VqdtcC36GJIN"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu5Zw2d8GJIP"},"outputs":[],"source":["# not the best way to preprocess features, but enough for the demonstration\n","# preprocess = sklearn.preprocessing.StandardScaler().fit(X_orig['train'])\n","preprocess = sklearn.preprocessing.QuantileTransformer().fit(X_orig['train'])\n","# preprocess = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1),clip=True).fit(X_orig['train'])\n","\n","X = {\n","    k: (2*preprocess.transform(v)-1)#preprocess.transform(v)\n","    for k, v in X_orig.items()\n","}\n","y = {k: v for k, v in y.items()}\n","\n","# !!! CRUCIAL for neural networks when solving regression problems !!!\n","if task_type == 'regression':\n","    y_mean = y['train'].mean().item()\n","    y_std = y['train'].std().item()\n","    y = {k: (v - y_mean) / y_std for k, v in y.items()}\n","else:\n","    y_std = y_mean = None\n","\n","# if task_type != 'multiclass':\n","#     y = {k: v.float() for k, v in y.items()}"],"id":"Yu5Zw2d8GJIP"},{"cell_type":"markdown","metadata":{"id":"eLx6kBZuGJIR"},"source":["### Side Notes: Neighborhood Components Analysis\n","Here we investigate which features help us relate the feature space to the target as if we were using kNN. 2 cells down, notice the last 2 columns are showing the highest weighting which indicates an optimal kNN distance to use should put more weight on Latitude and Longitude as opposed to other features in the models."],"id":"eLx6kBZuGJIR"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"TlGRXHCRGJIU","executionInfo":{"status":"error","timestamp":1698869829477,"user_tz":240,"elapsed":325231,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"b5a79601-7a14-4166-f8a6-9815e74e8405"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-26e27f4b7cd6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeighborhoodComponentsAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_nca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# Call the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mopt_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptimizer_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Reshape the solution found by the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                  **options)\n\u001b[1;32m    709\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    711\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    712\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_nca.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[0;34m(self, transformation, X, same_class_mask, sign)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mp_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ij\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mp_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp_ij\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (n_samples, n_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","from sklearn.neighbors import NeighborhoodComponentsAnalysis\n","\n","nca = NeighborhoodComponentsAnalysis(random_state=42)\n","nca.fit(X['train'], np.clip(np.round(1.5*y['train']),-2,3))\n","\n","np.matmul(nca.components_,np.transpose(nca.components_))"],"id":"TlGRXHCRGJIU"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2Ep-yxGGJIW"},"outputs":[],"source":["x_nca = np.matmul(nca.components_,np.transpose(nca.components_))\n","print(np.array_str(x_nca/1000., precision=3, suppress_small=True))"],"id":"c2Ep-yxGGJIW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"51XDD4uAGJIY"},"outputs":[],"source":["#create categories for common numeric values (not useful here)\n","\n","# min_support=15\n","# for ii in range(8):\n","#     values = pd.DataFrame(X['train'])[ii].value_counts()\n","#     values=np.sort(values[values>=min_support].index)\n","#     if len(values)>0:\n","#         enc = OneHotEncoder(categories=[list(values)],handle_unknown='ignore')\n","#         X['train']=np.concatenate([X['train'],enc.fit_transform(pd.DataFrame(X['train'])[[ii]]).toarray()],axis=1)\n","#         X['test']=np.concatenate([X['test'],enc.fit_transform(pd.DataFrame(X['test'])[[ii]]).toarray()],axis=1)\n","#         X['val']=np.concatenate([X['val'],enc.fit_transform(pd.DataFrame(X['val'])[[ii]]).toarray()],axis=1)"],"id":"51XDD4uAGJIY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"86IMfz08GJIZ"},"outputs":[],"source":["X['train'].shape, X['test'].shape, X['val'].shape"],"id":"86IMfz08GJIZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bn6iJB_sGJIa"},"outputs":[],"source":["#we standardize the target based on the training portion\n","np.sqrt(np.mean(y['val']*y['val']))"],"id":"bn6iJB_sGJIa"},{"cell_type":"markdown","metadata":{"id":"_A-YCdGmGJIc"},"source":["### Model Training / Experiments"],"id":"_A-YCdGmGJIc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9L86qAHEGJIa"},"outputs":[],"source":["epochs = 25\n","batch_size=128\n","init_lr = 0.001\n","\n","#we dont need these, but can be useful with certain learning rate schedulers\n","# steps_per_epoch = int(len(X['train'])/batch_size)\n","# num_train_steps = steps_per_epoch * epochs\n","# num_warmup_steps = 0"],"id":"9L86qAHEGJIa"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nT8ZAFqeGJId","executionInfo":{"status":"ok","timestamp":1699492678368,"user_tz":300,"elapsed":30805,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"7fdfd774-5a6c-4905-f129-bc02c95c6edc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Gradients do not exist for variables ['mlp/dense/kernel:0', 'mlp/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['mlp/dense/kernel:0', 'mlp/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"]},{"output_type":"stream","name":"stdout","text":["101/104 [============================>.] - ETA: 0s - loss: 0.3667 - mse: 0.3667\n","Epoch 1: val_loss improved from inf to 0.31499, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 4s 15ms/step - loss: 0.3649 - mse: 0.3649 - val_loss: 0.3150 - val_mse: 0.3150\n","Epoch 2/25\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2864 - mse: 0.2864\n","Epoch 2: val_loss improved from 0.31499 to 0.28951, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2840 - mse: 0.2840 - val_loss: 0.2895 - val_mse: 0.2895\n","Epoch 3/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2674 - mse: 0.2674\n","Epoch 3: val_loss improved from 0.28951 to 0.28575, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2677 - mse: 0.2677 - val_loss: 0.2858 - val_mse: 0.2858\n","Epoch 4/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2612 - mse: 0.2612\n","Epoch 4: val_loss improved from 0.28575 to 0.27148, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2605 - mse: 0.2605 - val_loss: 0.2715 - val_mse: 0.2715\n","Epoch 5/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2485 - mse: 0.2485\n","Epoch 5: val_loss improved from 0.27148 to 0.26631, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2491 - mse: 0.2491 - val_loss: 0.2663 - val_mse: 0.2663\n","Epoch 6/25\n","104/104 [==============================] - ETA: 0s - loss: 0.2441 - mse: 0.2441\n","Epoch 6: val_loss improved from 0.26631 to 0.25303, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2441 - mse: 0.2441 - val_loss: 0.2530 - val_mse: 0.2530\n","Epoch 7/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2414 - mse: 0.2414\n","Epoch 7: val_loss did not improve from 0.25303\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2566 - val_mse: 0.2566\n","Epoch 8/25\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2335 - mse: 0.2335\n","Epoch 8: val_loss improved from 0.25303 to 0.24787, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2328 - mse: 0.2328 - val_loss: 0.2479 - val_mse: 0.2479\n","Epoch 9/25\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2296 - mse: 0.2296\n","Epoch 9: val_loss improved from 0.24787 to 0.24581, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2319 - mse: 0.2319 - val_loss: 0.2458 - val_mse: 0.2458\n","Epoch 10/25\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2297 - mse: 0.2297\n","Epoch 10: val_loss did not improve from 0.24581\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2279 - mse: 0.2279 - val_loss: 0.2531 - val_mse: 0.2531\n","Epoch 11/25\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2265 - mse: 0.2265\n","Epoch 11: val_loss improved from 0.24581 to 0.23766, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2265 - mse: 0.2265 - val_loss: 0.2377 - val_mse: 0.2377\n","Epoch 12/25\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.2227 - mse: 0.2227\n","Epoch 12: val_loss did not improve from 0.23766\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2227 - mse: 0.2227 - val_loss: 0.2430 - val_mse: 0.2430\n","Epoch 13/25\n","103/104 [============================>.] - ETA: 0s - loss: 0.2211 - mse: 0.2211\n","Epoch 13: val_loss did not improve from 0.23766\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2209 - mse: 0.2209 - val_loss: 0.2410 - val_mse: 0.2410\n","Epoch 14/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2182 - mse: 0.2182\n","Epoch 14: val_loss improved from 0.23766 to 0.23579, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2175 - mse: 0.2175 - val_loss: 0.2358 - val_mse: 0.2358\n","Epoch 15/25\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2165 - mse: 0.2165\n","Epoch 15: val_loss did not improve from 0.23579\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2163 - mse: 0.2163 - val_loss: 0.2373 - val_mse: 0.2373\n","Epoch 16/25\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2176 - mse: 0.2176\n","Epoch 16: val_loss improved from 0.23579 to 0.23106, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2156 - mse: 0.2156 - val_loss: 0.2311 - val_mse: 0.2311\n","Epoch 17/25\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.2105 - mse: 0.2105\n","Epoch 17: val_loss did not improve from 0.23106\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2106 - mse: 0.2106 - val_loss: 0.2313 - val_mse: 0.2313\n","Epoch 18/25\n","101/104 [============================>.] - ETA: 0s - loss: 0.2120 - mse: 0.2120\n","Epoch 18: val_loss improved from 0.23106 to 0.22990, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2118 - mse: 0.2118 - val_loss: 0.2299 - val_mse: 0.2299\n","Epoch 19/25\n","102/104 [============================>.] - ETA: 0s - loss: 0.2117 - mse: 0.2117\n","Epoch 19: val_loss improved from 0.22990 to 0.22576, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2109 - mse: 0.2109 - val_loss: 0.2258 - val_mse: 0.2258\n","Epoch 20/25\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2041 - mse: 0.2041\n","Epoch 20: val_loss improved from 0.22576 to 0.22360, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2050 - mse: 0.2050 - val_loss: 0.2236 - val_mse: 0.2236\n","Epoch 21/25\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2056 - mse: 0.2056\n","Epoch 21: val_loss did not improve from 0.22360\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2061 - mse: 0.2061 - val_loss: 0.2346 - val_mse: 0.2346\n","Epoch 22/25\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.2079 - mse: 0.2079\n","Epoch 22: val_loss did not improve from 0.22360\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2103 - mse: 0.2103 - val_loss: 0.2280 - val_mse: 0.2280\n","Epoch 23/25\n","104/104 [==============================] - ETA: 0s - loss: 0.2050 - mse: 0.2050\n","Epoch 23: val_loss did not improve from 0.22360\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2050 - mse: 0.2050 - val_loss: 0.2421 - val_mse: 0.2421\n","Epoch 24/25\n","103/104 [============================>.] - ETA: 0s - loss: 0.2030 - mse: 0.2030\n","Epoch 24: val_loss did not improve from 0.22360\n","104/104 [==============================] - 1s 5ms/step - loss: 0.2029 - mse: 0.2029 - val_loss: 0.2330 - val_mse: 0.2330\n","Epoch 25/25\n"," 90/104 [========================>.....] - ETA: 0s - loss: 0.1995 - mse: 0.1995\n","Epoch 25: val_loss did not improve from 0.22360\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2020 - mse: 0.2020 - val_loss: 0.2287 - val_mse: 0.2287\n","104/104 [==============================] - 0s 2ms/step - loss: 0.2163 - mse: 0.2163\n","[0.46509971 0.46509971]\n","104/104 [==============================] - 0s 2ms/step - loss: 0.2101 - mse: 0.2101\n","[0.45833664 0.45833664]\n"]}],"source":["# Setup checkpoint path (to save the best weights / reduce overfitting)\n","checkpoint_path = \"model_checkpoint/checkpoint.ckpt\"\n","\n","# Create a ModelCheckpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                         save_weights_only=True, # set to False to save the entire model\n","                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch\n","                                                         save_freq=\"epoch\", # save every epoch\n","                                                         verbose=1)\n","this_model=MLP(512,512)\n","this_model.compile(optimizer=tfa.optimizers.AdamW(0,.003), loss='MSE',   metrics=['mse'])#tf.keras.losses.MSE\n","this_model.fit(X['train'], y['train'],validation_data=(X['test'], y['test']), batch_size=batch_size,epochs=epochs,  callbacks=[checkpoint_callback])\n","print(np.sqrt(this_model.evaluate(X['val'], y['val'])))\n","this_model.load_weights(checkpoint_path)\n","print(np.sqrt(this_model.evaluate(X['val'], y['val'])))"],"id":"nT8ZAFqeGJId"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6St2-Z8GJIf","executionInfo":{"status":"ok","timestamp":1699493224030,"user_tz":300,"elapsed":17598,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"f0718daa-7f80-49b8-ba88-52adbebd5168"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 Learning Rate: 0.00125\n","Epoch 1/35\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Gradients do not exist for variables ['res_net_dr/dense_3/kernel:0', 'res_net_dr/dense_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['res_net_dr/dense_3/kernel:0', 'res_net_dr/dense_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"]},{"output_type":"stream","name":"stdout","text":[" 98/104 [===========================>..] - ETA: 0s - loss: 0.4666 - mse: 0.4666\n","Epoch 1: val_loss improved from inf to 0.35566, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.4587 - mse: 0.4587 - val_loss: 0.3557 - val_mse: 0.3557 - lr: 0.0012\n","Epoch: 1 Learning Rate: 0.0025\n","Epoch 2/35\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.3095 - mse: 0.3095\n","Epoch 2: val_loss improved from 0.35566 to 0.30825, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 5ms/step - loss: 0.3077 - mse: 0.3077 - val_loss: 0.3082 - val_mse: 0.3082 - lr: 0.0025\n","Epoch: 2 Learning Rate: 0.005\n","Epoch 3/35\n"," 89/104 [========================>.....] - ETA: 0s - loss: 0.2922 - mse: 0.2922\n","Epoch 3: val_loss improved from 0.30825 to 0.29400, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.2940 - val_mse: 0.2940 - lr: 0.0050\n","Epoch: 3 Learning Rate: 0.01\n","Epoch 4/35\n"," 86/104 [=======================>......] - ETA: 0s - loss: 0.2828 - mse: 0.2828\n","Epoch 4: val_loss did not improve from 0.29400\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2805 - mse: 0.2805 - val_loss: 0.3105 - val_mse: 0.3105 - lr: 0.0100\n","Epoch: 4 Learning Rate: 0.01\n","Epoch 5/35\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.2720 - mse: 0.2720\n","Epoch 5: val_loss improved from 0.29400 to 0.27009, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2699 - mse: 0.2699 - val_loss: 0.2701 - val_mse: 0.2701 - lr: 0.0100\n","Epoch: 5 Learning Rate: 0.01\n","Epoch 6/35\n"," 90/104 [========================>.....] - ETA: 0s - loss: 0.2504 - mse: 0.2504\n","Epoch 6: val_loss did not improve from 0.27009\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2543 - mse: 0.2543 - val_loss: 0.2880 - val_mse: 0.2880 - lr: 0.0100\n","Epoch: 6 Learning Rate: 0.01\n","Epoch 7/35\n"," 93/104 [=========================>....] - ETA: 0s - loss: 0.2486 - mse: 0.2486\n","Epoch 7: val_loss improved from 0.27009 to 0.26628, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2493 - mse: 0.2493 - val_loss: 0.2663 - val_mse: 0.2663 - lr: 0.0100\n","Epoch: 7 Learning Rate: 0.005\n","Epoch 8/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2304 - mse: 0.2304\n","Epoch 8: val_loss improved from 0.26628 to 0.24336, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2302 - mse: 0.2302 - val_loss: 0.2434 - val_mse: 0.2434 - lr: 0.0050\n","Epoch: 8 Learning Rate: 0.005\n","Epoch 9/35\n"," 90/104 [========================>.....] - ETA: 0s - loss: 0.2260 - mse: 0.2260\n","Epoch 9: val_loss did not improve from 0.24336\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2249 - mse: 0.2249 - val_loss: 0.2484 - val_mse: 0.2484 - lr: 0.0050\n","Epoch: 9 Learning Rate: 0.005\n","Epoch 10/35\n"," 87/104 [========================>.....] - ETA: 0s - loss: 0.2229 - mse: 0.2229\n","Epoch 10: val_loss did not improve from 0.24336\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2248 - mse: 0.2248 - val_loss: 0.2512 - val_mse: 0.2512 - lr: 0.0050\n","Epoch: 10 Learning Rate: 0.005\n","Epoch 11/35\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.2228 - mse: 0.2228\n","Epoch 11: val_loss did not improve from 0.24336\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2239 - mse: 0.2239 - val_loss: 0.2436 - val_mse: 0.2436 - lr: 0.0050\n","Epoch: 11 Learning Rate: 0.005\n","Epoch 12/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2215 - mse: 0.2215\n","Epoch 12: val_loss improved from 0.24336 to 0.23860, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2198 - mse: 0.2198 - val_loss: 0.2386 - val_mse: 0.2386 - lr: 0.0050\n","Epoch: 12 Learning Rate: 0.005\n","Epoch 13/35\n"," 91/104 [=========================>....] - ETA: 0s - loss: 0.2229 - mse: 0.2229\n","Epoch 13: val_loss did not improve from 0.23860\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2216 - mse: 0.2216 - val_loss: 0.2392 - val_mse: 0.2392 - lr: 0.0050\n","Epoch: 13 Learning Rate: 0.005\n","Epoch 14/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2204 - mse: 0.2204\n","Epoch 14: val_loss did not improve from 0.23860\n","104/104 [==============================] - 0s 5ms/step - loss: 0.2203 - mse: 0.2203 - val_loss: 0.2416 - val_mse: 0.2416 - lr: 0.0050\n","Epoch: 14 Learning Rate: 0.005\n","Epoch 15/35\n"," 87/104 [========================>.....] - ETA: 0s - loss: 0.2142 - mse: 0.2142\n","Epoch 15: val_loss did not improve from 0.23860\n","104/104 [==============================] - 0s 5ms/step - loss: 0.2177 - mse: 0.2177 - val_loss: 0.2765 - val_mse: 0.2765 - lr: 0.0050\n","Epoch: 15 Learning Rate: 0.0025\n","Epoch 16/35\n"," 86/104 [=======================>......] - ETA: 0s - loss: 0.2150 - mse: 0.2150\n","Epoch 16: val_loss improved from 0.23860 to 0.22885, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2122 - mse: 0.2122 - val_loss: 0.2288 - val_mse: 0.2288 - lr: 0.0025\n","Epoch: 16 Learning Rate: 0.0025\n","Epoch 17/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2042 - mse: 0.2042\n","Epoch 17: val_loss did not improve from 0.22885\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2042 - mse: 0.2042 - val_loss: 0.2296 - val_mse: 0.2296 - lr: 0.0025\n","Epoch: 17 Learning Rate: 0.0025\n","Epoch 18/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2036 - mse: 0.2036\n","Epoch 18: val_loss did not improve from 0.22885\n","104/104 [==============================] - 0s 4ms/step - loss: 0.2040 - mse: 0.2040 - val_loss: 0.2291 - val_mse: 0.2291 - lr: 0.0025\n","Epoch: 18 Learning Rate: 0.0025\n","Epoch 19/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.2058 - mse: 0.2058\n","Epoch 19: val_loss did not improve from 0.22885\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2050 - mse: 0.2050 - val_loss: 0.2327 - val_mse: 0.2327 - lr: 0.0025\n","Epoch: 19 Learning Rate: 0.0025\n","Epoch 20/35\n"," 92/104 [=========================>....] - ETA: 0s - loss: 0.2010 - mse: 0.2010\n","Epoch 20: val_loss did not improve from 0.22885\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2030 - mse: 0.2030 - val_loss: 0.2312 - val_mse: 0.2312 - lr: 0.0025\n","Epoch: 20 Learning Rate: 0.0025\n","Epoch 21/35\n"," 95/104 [==========================>...] - ETA: 0s - loss: 0.2015 - mse: 0.2015\n","Epoch 21: val_loss did not improve from 0.22885\n","104/104 [==============================] - 1s 6ms/step - loss: 0.2021 - mse: 0.2021 - val_loss: 0.2369 - val_mse: 0.2369 - lr: 0.0025\n","Epoch: 21 Learning Rate: 0.0025\n","Epoch 22/35\n"," 96/104 [==========================>...] - ETA: 0s - loss: 0.2012 - mse: 0.2012\n","Epoch 22: val_loss improved from 0.22885 to 0.22462, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 7ms/step - loss: 0.2003 - mse: 0.2003 - val_loss: 0.2246 - val_mse: 0.2246 - lr: 0.0025\n","Epoch: 22 Learning Rate: 0.0025\n","Epoch 23/35\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.1987 - mse: 0.1987\n","Epoch 23: val_loss improved from 0.22462 to 0.22439, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 6ms/step - loss: 0.1994 - mse: 0.1994 - val_loss: 0.2244 - val_mse: 0.2244 - lr: 0.0025\n","Epoch: 23 Learning Rate: 0.00125\n","Epoch 24/35\n"," 92/104 [=========================>....] - ETA: 0s - loss: 0.1963 - mse: 0.1963\n","Epoch 24: val_loss improved from 0.22439 to 0.22340, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1949 - mse: 0.1949 - val_loss: 0.2234 - val_mse: 0.2234 - lr: 0.0012\n","Epoch: 24 Learning Rate: 0.00125\n","Epoch 25/35\n"," 94/104 [==========================>...] - ETA: 0s - loss: 0.1941 - mse: 0.1941\n","Epoch 25: val_loss improved from 0.22340 to 0.21974, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1938 - mse: 0.1938 - val_loss: 0.2197 - val_mse: 0.2197 - lr: 0.0012\n","Epoch: 25 Learning Rate: 0.00125\n","Epoch 26/35\n"," 86/104 [=======================>......] - ETA: 0s - loss: 0.1928 - mse: 0.1928\n","Epoch 26: val_loss did not improve from 0.21974\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1931 - mse: 0.1931 - val_loss: 0.2207 - val_mse: 0.2207 - lr: 0.0012\n","Epoch: 26 Learning Rate: 0.00125\n","Epoch 27/35\n"," 94/104 [==========================>...] - ETA: 0s - loss: 0.1897 - mse: 0.1897\n","Epoch 27: val_loss improved from 0.21974 to 0.21865, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.2187 - val_mse: 0.2187 - lr: 0.0012\n","Epoch: 27 Learning Rate: 0.00125\n","Epoch 28/35\n"," 97/104 [==========================>...] - ETA: 0s - loss: 0.1917 - mse: 0.1917\n","Epoch 28: val_loss did not improve from 0.21865\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1912 - mse: 0.1912 - val_loss: 0.2204 - val_mse: 0.2204 - lr: 0.0012\n","Epoch: 28 Learning Rate: 0.00125\n","Epoch 29/35\n"," 92/104 [=========================>....] - ETA: 0s - loss: 0.1901 - mse: 0.1901\n","Epoch 29: val_loss did not improve from 0.21865\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.2240 - val_mse: 0.2240 - lr: 0.0012\n","Epoch: 29 Learning Rate: 0.00125\n","Epoch 30/35\n","104/104 [==============================] - ETA: 0s - loss: 0.1908 - mse: 0.1908\n","Epoch 30: val_loss did not improve from 0.21865\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1908 - mse: 0.1908 - val_loss: 0.2268 - val_mse: 0.2268 - lr: 0.0012\n","Epoch: 30 Learning Rate: 0.00125\n","Epoch 31/35\n"," 92/104 [=========================>....] - ETA: 0s - loss: 0.1911 - mse: 0.1911\n","Epoch 31: val_loss improved from 0.21865 to 0.21846, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1903 - mse: 0.1903 - val_loss: 0.2185 - val_mse: 0.2185 - lr: 0.0012\n","Epoch: 31 Learning Rate: 0.000625\n","Epoch 32/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.1868 - mse: 0.1868\n","Epoch 32: val_loss improved from 0.21846 to 0.21735, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1862 - mse: 0.1862 - val_loss: 0.2173 - val_mse: 0.2173 - lr: 6.2500e-04\n","Epoch: 32 Learning Rate: 0.000625\n","Epoch 33/35\n"," 89/104 [========================>.....] - ETA: 0s - loss: 0.1825 - mse: 0.1825\n","Epoch 33: val_loss improved from 0.21735 to 0.21722, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1863 - mse: 0.1863 - val_loss: 0.2172 - val_mse: 0.2172 - lr: 6.2500e-04\n","Epoch: 33 Learning Rate: 0.000625\n","Epoch 34/35\n","104/104 [==============================] - ETA: 0s - loss: 0.1858 - mse: 0.1858\n","Epoch 34: val_loss improved from 0.21722 to 0.21719, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.2172 - val_mse: 0.2172 - lr: 6.2500e-04\n","Epoch: 34 Learning Rate: 0.000625\n","Epoch 35/35\n","104/104 [==============================] - ETA: 0s - loss: 0.1856 - mse: 0.1856\n","Epoch 35: val_loss did not improve from 0.21719\n","104/104 [==============================] - 0s 4ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.2188 - val_mse: 0.2188 - lr: 6.2500e-04\n","104/104 [==============================] - 0s 2ms/step - loss: 0.2063 - mse: 0.2063\n","[0.45423697 0.45423697]\n","104/104 [==============================] - 0s 2ms/step - loss: 0.2082 - mse: 0.2082\n","[0.45627582 0.45627582]\n"]}],"source":["epochs=35\n","\n","def warmup_and_step_decay(epoch):\n","    initial_lrate = 0.01\n","    drop = 0.5\n","    epochs_drop = 8\n","    warmup=.5\n","    warmup_steps=2\n","    if epoch<=warmup_steps:\n","        lrate = pow(warmup,warmup_steps-epoch+1)*initial_lrate\n","    else:\n","        lrate = initial_lrate * math.pow(drop,  math.floor((1+epoch)/epochs_drop))\n","    print(\"Epoch: \"+str(epoch)+\" Learning Rate: \"+str(lrate))\n","    return lrate\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(warmup_and_step_decay)\n","\n","# Setup checkpoint path\n","checkpoint_path = \"model_checkpoint/checkpoint.ckpt\"\n","\n","# Create a ModelCheckpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                         save_weights_only=True, # set to False to save the entire model\n","                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch\n","                                                         save_freq=\"epoch\", # save every epoch\n","                                                         verbose=1)\n","\n","ResModelDR = ResNetDR(256,256)\n","ResModelDR.compile(optimizer=tfa.optimizers.AdamW(0,.003), loss='MSE',   metrics=['mse'])#tf.keras.losses.MSE\n","ResModelDR.fit(X['train'], y['train'],validation_data=(X['test'], y['test']), batch_size=batch_size,epochs=epochs,  callbacks=[checkpoint_callback,lr_callback])\n","print(np.sqrt(ResModelDR.evaluate(X['val'], y['val'])))\n","ResModelDR.load_weights(checkpoint_path)\n","print(np.sqrt(ResModelDR.evaluate(X['val'], y['val'])))"],"id":"D6St2-Z8GJIf"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd1kRD5kGJIg","executionInfo":{"status":"ok","timestamp":1698869914687,"user_tz":240,"elapsed":49033,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"952a2178-1a4c-4b8d-d592-c278c43e948c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 Learning Rate: 0.00125\n","Epoch 1/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.4556 - mse: 0.4556\n","Epoch 1: val_loss improved from inf to 0.58543, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 4s 18ms/step - loss: 0.4556 - mse: 0.4556 - val_loss: 0.5854 - val_mse: 0.5854 - lr: 0.0012\n","Epoch: 1 Learning Rate: 0.0025\n","Epoch 2/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.3404 - mse: 0.3404\n","Epoch 2: val_loss improved from 0.58543 to 0.40567, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3415 - mse: 0.3415 - val_loss: 0.4057 - val_mse: 0.4057 - lr: 0.0025\n","Epoch: 2 Learning Rate: 0.005\n","Epoch 3/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.3177 - mse: 0.3177\n","Epoch 3: val_loss improved from 0.40567 to 0.34031, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3179 - mse: 0.3179 - val_loss: 0.3403 - val_mse: 0.3403 - lr: 0.0050\n","Epoch: 3 Learning Rate: 0.01\n","Epoch 4/35\n"," 99/104 [===========================>..] - ETA: 0s - loss: 0.3170 - mse: 0.3170\n","Epoch 4: val_loss did not improve from 0.34031\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3161 - mse: 0.3161 - val_loss: 0.4392 - val_mse: 0.4392 - lr: 0.0100\n","Epoch: 4 Learning Rate: 0.01\n","Epoch 5/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2895 - mse: 0.2895\n","Epoch 5: val_loss improved from 0.34031 to 0.32926, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2895 - mse: 0.2895 - val_loss: 0.3293 - val_mse: 0.3293 - lr: 0.0100\n","Epoch: 5 Learning Rate: 0.01\n","Epoch 6/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2715 - mse: 0.2715\n","Epoch 6: val_loss improved from 0.32926 to 0.32474, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2719 - mse: 0.2719 - val_loss: 0.3247 - val_mse: 0.3247 - lr: 0.0100\n","Epoch: 6 Learning Rate: 0.01\n","Epoch 7/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2739 - mse: 0.2739\n","Epoch 7: val_loss improved from 0.32474 to 0.27907, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2739 - mse: 0.2739 - val_loss: 0.2791 - val_mse: 0.2791 - lr: 0.0100\n","Epoch: 7 Learning Rate: 0.005\n","Epoch 8/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2486 - mse: 0.2486\n","Epoch 8: val_loss improved from 0.27907 to 0.24314, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2483 - mse: 0.2483 - val_loss: 0.2431 - val_mse: 0.2431 - lr: 0.0050\n","Epoch: 8 Learning Rate: 0.005\n","Epoch 9/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2533 - mse: 0.2533\n","Epoch 9: val_loss did not improve from 0.24314\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2518 - mse: 0.2518 - val_loss: 0.2515 - val_mse: 0.2515 - lr: 0.0050\n","Epoch: 9 Learning Rate: 0.005\n","Epoch 10/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2434 - mse: 0.2434\n","Epoch 10: val_loss improved from 0.24314 to 0.22962, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2434 - mse: 0.2434 - val_loss: 0.2296 - val_mse: 0.2296 - lr: 0.0050\n","Epoch: 10 Learning Rate: 0.005\n","Epoch 11/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2467 - mse: 0.2467\n","Epoch 11: val_loss improved from 0.22962 to 0.22351, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 0.2235 - val_mse: 0.2235 - lr: 0.0050\n","Epoch: 11 Learning Rate: 0.005\n","Epoch 12/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2461 - mse: 0.2461\n","Epoch 12: val_loss did not improve from 0.22351\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2464 - mse: 0.2464 - val_loss: 0.2484 - val_mse: 0.2484 - lr: 0.0050\n","Epoch: 12 Learning Rate: 0.005\n","Epoch 13/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2466 - mse: 0.2466\n","Epoch 13: val_loss did not improve from 0.22351\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2459 - mse: 0.2459 - val_loss: 0.2250 - val_mse: 0.2250 - lr: 0.0050\n","Epoch: 13 Learning Rate: 0.005\n","Epoch 14/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2372 - mse: 0.2372\n","Epoch 14: val_loss did not improve from 0.22351\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2371 - mse: 0.2371 - val_loss: 0.2344 - val_mse: 0.2344 - lr: 0.0050\n","Epoch: 14 Learning Rate: 0.005\n","Epoch 15/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2514 - mse: 0.2514\n","Epoch 15: val_loss did not improve from 0.22351\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.2627 - val_mse: 0.2627 - lr: 0.0050\n","Epoch: 15 Learning Rate: 0.0025\n","Epoch 16/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2328 - mse: 0.2328\n","Epoch 16: val_loss did not improve from 0.22351\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2311 - mse: 0.2311 - val_loss: 0.2287 - val_mse: 0.2287 - lr: 0.0025\n","Epoch: 16 Learning Rate: 0.0025\n","Epoch 17/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2271 - mse: 0.2271\n","Epoch 17: val_loss improved from 0.22351 to 0.21385, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2265 - mse: 0.2265 - val_loss: 0.2138 - val_mse: 0.2138 - lr: 0.0025\n","Epoch: 17 Learning Rate: 0.0025\n","Epoch 18/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2271 - mse: 0.2271\n","Epoch 18: val_loss did not improve from 0.21385\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2261 - mse: 0.2261 - val_loss: 0.2165 - val_mse: 0.2165 - lr: 0.0025\n","Epoch: 18 Learning Rate: 0.0025\n","Epoch 19/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2266 - mse: 0.2266\n","Epoch 19: val_loss did not improve from 0.21385\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2264 - mse: 0.2264 - val_loss: 0.2262 - val_mse: 0.2262 - lr: 0.0025\n","Epoch: 19 Learning Rate: 0.0025\n","Epoch 20/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2207 - mse: 0.2207\n","Epoch 20: val_loss did not improve from 0.21385\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2207 - mse: 0.2207 - val_loss: 0.2376 - val_mse: 0.2376 - lr: 0.0025\n","Epoch: 20 Learning Rate: 0.0025\n","Epoch 21/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2253 - mse: 0.2253\n","Epoch 21: val_loss improved from 0.21385 to 0.21117, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2264 - mse: 0.2264 - val_loss: 0.2112 - val_mse: 0.2112 - lr: 0.0025\n","Epoch: 21 Learning Rate: 0.0025\n","Epoch 22/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2201 - mse: 0.2201\n","Epoch 22: val_loss improved from 0.21117 to 0.20563, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2209 - mse: 0.2209 - val_loss: 0.2056 - val_mse: 0.2056 - lr: 0.0025\n","Epoch: 22 Learning Rate: 0.0025\n","Epoch 23/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2208 - mse: 0.2208\n","Epoch 23: val_loss did not improve from 0.20563\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2201 - mse: 0.2201 - val_loss: 0.2100 - val_mse: 0.2100 - lr: 0.0025\n","Epoch: 23 Learning Rate: 0.00125\n","Epoch 24/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2131 - mse: 0.2131\n","Epoch 24: val_loss improved from 0.20563 to 0.20004, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2127 - mse: 0.2127 - val_loss: 0.2000 - val_mse: 0.2000 - lr: 0.0012\n","Epoch: 24 Learning Rate: 0.00125\n","Epoch 25/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2098 - mse: 0.2098\n","Epoch 25: val_loss did not improve from 0.20004\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2094 - mse: 0.2094 - val_loss: 0.2050 - val_mse: 0.2050 - lr: 0.0012\n","Epoch: 25 Learning Rate: 0.00125\n","Epoch 26/35\n","103/104 [============================>.] - ETA: 0s - loss: 0.2182 - mse: 0.2182\n","Epoch 26: val_loss did not improve from 0.20004\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2181 - mse: 0.2181 - val_loss: 0.2033 - val_mse: 0.2033 - lr: 0.0012\n","Epoch: 26 Learning Rate: 0.00125\n","Epoch 27/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2113 - mse: 0.2113\n","Epoch 27: val_loss did not improve from 0.20004\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2113 - mse: 0.2113 - val_loss: 0.2015 - val_mse: 0.2015 - lr: 0.0012\n","Epoch: 27 Learning Rate: 0.00125\n","Epoch 28/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2123 - mse: 0.2123\n","Epoch 28: val_loss improved from 0.20004 to 0.19936, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2119 - mse: 0.2119 - val_loss: 0.1994 - val_mse: 0.1994 - lr: 0.0012\n","Epoch: 28 Learning Rate: 0.00125\n","Epoch 29/35\n"," 98/104 [===========================>..] - ETA: 0s - loss: 0.2142 - mse: 0.2142\n","Epoch 29: val_loss did not improve from 0.19936\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2120 - mse: 0.2120 - val_loss: 0.2155 - val_mse: 0.2155 - lr: 0.0012\n","Epoch: 29 Learning Rate: 0.00125\n","Epoch 30/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2107 - mse: 0.2107\n","Epoch 30: val_loss did not improve from 0.19936\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2109 - mse: 0.2109 - val_loss: 0.1995 - val_mse: 0.1995 - lr: 0.0012\n","Epoch: 30 Learning Rate: 0.00125\n","Epoch 31/35\n","104/104 [==============================] - ETA: 0s - loss: 0.2113 - mse: 0.2113\n","Epoch 31: val_loss improved from 0.19936 to 0.19857, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2113 - mse: 0.2113 - val_loss: 0.1986 - val_mse: 0.1986 - lr: 0.0012\n","Epoch: 31 Learning Rate: 0.000625\n","Epoch 32/35\n","100/104 [===========================>..] - ETA: 0s - loss: 0.2038 - mse: 0.2038\n","Epoch 32: val_loss did not improve from 0.19857\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2032 - mse: 0.2032 - val_loss: 0.1996 - val_mse: 0.1996 - lr: 6.2500e-04\n","Epoch: 32 Learning Rate: 0.000625\n","Epoch 33/35\n","101/104 [============================>.] - ETA: 0s - loss: 0.2003 - mse: 0.2003\n","Epoch 33: val_loss improved from 0.19857 to 0.19351, saving model to model_checkpoint/checkpoint.ckpt\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2004 - mse: 0.2004 - val_loss: 0.1935 - val_mse: 0.1935 - lr: 6.2500e-04\n","Epoch: 33 Learning Rate: 0.000625\n","Epoch 34/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2029 - mse: 0.2029\n","Epoch 34: val_loss did not improve from 0.19351\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2038 - mse: 0.2038 - val_loss: 0.2029 - val_mse: 0.2029 - lr: 6.2500e-04\n","Epoch: 34 Learning Rate: 0.000625\n","Epoch 35/35\n","102/104 [============================>.] - ETA: 0s - loss: 0.2025 - mse: 0.2025\n","Epoch 35: val_loss did not improve from 0.19351\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2027 - mse: 0.2027 - val_loss: 0.1972 - val_mse: 0.1972 - lr: 6.2500e-04\n","104/104 [==============================] - 0s 3ms/step - loss: 0.2070 - mse: 0.2070\n","[0.45501728 0.45501728]\n","104/104 [==============================] - 0s 2ms/step - loss: 0.2037 - mse: 0.2037\n","[0.45137741 0.45137741]\n"]}],"source":["epochs=35\n","\n","def warmup_and_step_decay(epoch):\n","    initial_lrate = 0.01\n","    drop = 0.5\n","    epochs_drop = 8\n","    warmup=.5\n","    warmup_steps=2\n","    if epoch<=warmup_steps:\n","        lrate = pow(warmup,warmup_steps-epoch+1)*initial_lrate\n","    else:\n","        lrate = initial_lrate * math.pow(drop,  math.floor((1+epoch)/epochs_drop))\n","    print(\"Epoch: \"+str(epoch)+\" Learning Rate: \"+str(lrate))\n","    return lrate\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(warmup_and_step_decay)\n","\n","# Setup checkpoint path\n","checkpoint_path = \"model_checkpoint/checkpoint.ckpt\"\n","\n","# Create a ModelCheckpoint callback that saves the model's weights only\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                         save_weights_only=True, # set to False to save the entire model\n","                                                         save_best_only=True, # set to True to save only the best model instead of a model every epoch\n","                                                         save_freq=\"epoch\", # save every epoch\n","                                                         verbose=1)\n","\n","ResModel = ResNet(128,256)\n","ResModel.compile(optimizer=tfa.optimizers.AdamW(0,.003), loss='MSE',   metrics=['mse'])#tf.keras.losses.MSE\n","ResModel.fit(X['train'], y['train'],validation_data=(X['test'], y['test']), batch_size=batch_size,epochs=epochs,  callbacks=[checkpoint_callback,lr_callback])\n","print(np.sqrt(ResModel.evaluate(X['val'], y['val'])))\n","ResModel.load_weights(checkpoint_path)\n","print(np.sqrt(ResModel.evaluate(X['val'], y['val'])))"],"id":"rd1kRD5kGJIg"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSmlyPjuGJIj","executionInfo":{"status":"ok","timestamp":1698869914687,"user_tz":240,"elapsed":23,"user":{"displayName":"Joe Burdis","userId":"05388749857082436046"}},"outputId":"ea11d0a5-149d-4933-e8aa-95fa6730893c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"res_net\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_7 (Dense)             multiple                  1152      \n","                                                                 \n"," res_net_block (ResNetBlock  multiple                  66432     \n"," )                                                               \n","                                                                 \n"," res_net_block_1 (ResNetBlo  multiple                  66432     \n"," ck)                                                             \n","                                                                 \n"," batch_normalization_2 (Bat  multiple                  512       \n"," chNormalization)                                                \n","                                                                 \n"," p_re_lu_2 (PReLU)           multiple                  128       \n","                                                                 \n"," dense_12 (Dense)            multiple                  129       \n","                                                                 \n","=================================================================\n","Total params: 134785 (526.50 KB)\n","Trainable params: 134017 (523.50 KB)\n","Non-trainable params: 768 (3.00 KB)\n","_________________________________________________________________\n"]}],"source":["ResModel.summary()"],"id":"mSmlyPjuGJIj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"58YkSy4fGJIk"},"outputs":[],"source":[],"id":"58YkSy4fGJIk"}],"metadata":{"kernelspec":{"display_name":"py38","language":"python","name":"py38"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}